# Llama-3.1-8B â€” one transformer layer's GEMM workload
# 4 attention projections + 3 FFN matmuls = 7 ops (dense model, no MoE)
#
# Architecture:
#   hidden_size: 4096
#   intermediate_size: 14336
#   num_attention_heads: 32
#   num_key_value_heads: 8 (GQA)
#   head_dim: 128
#
# Format:
#   mul_mat <label> <M> <N> <K>

model_name  llama-3.1-8b
seq_len     512

mul_mat     attn_q        512  4096   4096
mul_mat     attn_k        512  1024   4096
mul_mat     attn_v        512  1024   4096
mul_mat     attn_output   512  4096   4096
mul_mat     ffn_gate      512  14336  4096
mul_mat     ffn_up        512  14336  4096
mul_mat     ffn_down      512  4096   14336
