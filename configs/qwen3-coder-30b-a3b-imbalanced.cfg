# Qwen3-Coder-30B-A3B — one transformer layer with REALISTIC IMBALANCED routing
# 4 attention projections + 1 gate routing + 3 MoE expert matmuls = 8 ops
# 128 total experts, 8 active per token
#
# This config uses realistic per-expert token distributions with 52x imbalance
# (power-law distribution typical of production LLM routing).
#
# WARNING: ~3.4 GB weight data at f16 per layer
#
# Format:
#   mul_mat      <label>  <M> <N> <K>
#   mul_mat_id   <label>  <M> <N> <K> <n_experts> <n_experts_used>
#   expert_tokens  <comma-separated counts> (must sum to M × n_used)

model_name  qwen3-coder-30b-a3b-imbalanced
seq_len     512

# Attention projections (no MoE, uniform load)
mul_mat     attn_q        512  4096   3072
mul_mat     attn_k        512  512    3072
mul_mat     attn_v        512  512    3072
mul_mat     attn_output   512  3072   4096

# FFN gating (selects which 8 of 128 experts to use)
mul_mat     ffn_gate_inp  512  128    3072

# MoE expert matmuls with realistic imbalanced routing
# Total: 4096 token-expert pairs (512 tokens × 8 experts/token)
# Distribution follows power law: max=468, min=9, imbalance=52x

mul_mat_id  ffn_gate_exp  512  1536   3072   128  8
expert_tokens 468, 269, 194, 155, 129, 112, 99, 89, 81, 75, 69, 64, 61, 57, 54, 51, 49, 47, 45, 43, 41, 40, 39, 37, 36, 35, 34, 33, 32, 31, 30, 30, 29, 28, 28, 27, 26, 26, 25, 25, 24, 24, 24, 23, 23, 22, 22, 22, 21, 21, 21, 20, 20, 20, 19, 19, 19, 19, 18, 18, 18, 18, 17, 17, 17, 17, 16, 15, 15, 15, 15, 15, 15, 14, 14, 14, 14, 14, 14, 14, 13, 13, 13, 13, 13, 13, 13, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 9, 9, 9, 9, 9, 9

mul_mat_id  ffn_up_exp    512  1536   3072   128  8
expert_tokens 468, 269, 194, 155, 129, 112, 99, 89, 81, 75, 69, 64, 61, 57, 54, 51, 49, 47, 45, 43, 41, 40, 39, 37, 36, 35, 34, 33, 32, 31, 30, 30, 29, 28, 28, 27, 26, 26, 25, 25, 24, 24, 24, 23, 23, 22, 22, 22, 21, 21, 21, 20, 20, 20, 19, 19, 19, 19, 18, 18, 18, 18, 17, 17, 17, 17, 16, 15, 15, 15, 15, 15, 15, 14, 14, 14, 14, 14, 14, 14, 13, 13, 13, 13, 13, 13, 13, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 9, 9, 9, 9, 9, 9

mul_mat_id  ffn_down_exp  512  3072   1536   128  8
expert_tokens 468, 269, 194, 155, 129, 112, 99, 89, 81, 75, 69, 64, 61, 57, 54, 51, 49, 47, 45, 43, 41, 40, 39, 37, 36, 35, 34, 33, 32, 31, 30, 30, 29, 28, 28, 27, 26, 26, 25, 25, 24, 24, 24, 23, 23, 22, 22, 22, 21, 21, 21, 20, 20, 20, 19, 19, 19, 19, 18, 18, 18, 18, 17, 17, 17, 17, 16, 15, 15, 15, 15, 15, 15, 14, 14, 14, 14, 14, 14, 14, 13, 13, 13, 13, 13, 13, 13, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 9, 9, 9, 9, 9, 9
