# Qwen2-7B â€” one transformer layer's GEMM workload
# 4 attention projections + 3 FFN matmuls = 7 ops (dense model, no MoE)
#
# Architecture:
#   hidden_size: 3584
#   intermediate_size: 18944
#   num_attention_heads: 28
#   num_key_value_heads: 4 (GQA)
#   head_dim: 128
#
# Format:
#   mul_mat <label> <M> <N> <K>

model_name  qwen2-7b
seq_len     512

mul_mat     attn_q        512  3584   3584
mul_mat     attn_k        512  512    3584
mul_mat     attn_v        512  512    3584
mul_mat     attn_output   512  3584   3584
mul_mat     ffn_gate      512  18944  3584
mul_mat     ffn_up        512  18944  3584
mul_mat     ffn_down      512  3584   18944
