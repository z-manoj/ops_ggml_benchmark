# Mixtral-8x7B-Instruct â€” one transformer layer's GEMM workload
# 4 attention projections + 1 gate routing + 3 MoE expert matmuls = 8 ops
#
# Format:
#   mul_mat      <label>  <M> <N> <K>
#   mul_mat_id   <label>  <M> <N> <K> <n_experts> <n_experts_used>

model_name  mixtral-8x7b-instruct
seq_len     512

mul_mat     attn_q        512  4096   4096
mul_mat     attn_k        512  1024   4096
mul_mat     attn_v        512  1024   4096
mul_mat     attn_output   512  4096   4096
mul_mat     ffn_gate_inp  512  8      4096
mul_mat_id  ffn_gate_exp  512  14336  4096   8  2
mul_mat_id  ffn_up_exp    512  14336  4096   8  2
mul_mat_id  ffn_down_exp  512  4096   14336  8  2
