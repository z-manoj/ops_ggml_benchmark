# Qwen3-Coder-30B-A3B — one transformer layer's GEMM workload
# 4 attention projections + 1 gate routing + 3 MoE expert matmuls = 8 ops
# 128 total experts, 8 active per token
#
# WARNING: ~108 GB weight data at f16 — requires large-memory machine.
#
# Format:
#   mul_mat      <label>  <M> <N> <K>
#   mul_mat_id   <label>  <M> <N> <K> <n_experts> <n_experts_used>

model_name  qwen3-coder-30b-a3b
seq_len     512

mul_mat     attn_q        512  4096   3072
mul_mat     attn_k        512  512    3072
mul_mat     attn_v        512  512    3072
mul_mat     attn_output   512  3072   4096
mul_mat     ffn_gate_inp  512  128    3072
mul_mat_id  ffn_gate_exp  512  1536   3072   128  8
mul_mat_id  ffn_up_exp    512  1536   3072   128  8
mul_mat_id  ffn_down_exp  512  3072   1536   128  8
