# Mixtral-8x7B-Instruct — one transformer layer with REALISTIC IMBALANCED routing
# 4 attention projections + 1 gate routing + 3 MoE expert matmuls = 8 ops
#
# This config uses realistic per-expert token distributions based on actual
# Mixtral routing behavior in production. Expert load varies 20x (typical).
#
# Format:
#   mul_mat      <label>  <M> <N> <K>
#   mul_mat_id   <label>  <M> <N> <K> <n_experts> <n_experts_used>
#   expert_tokens  <comma-separated counts> (must sum to M)

model_name  mixtral-8x7b-instruct-imbalanced
seq_len     512

# Attention projections (no MoE, uniform load)
mul_mat     attn_q        512  4096   4096
mul_mat     attn_k        512  1024   4096
mul_mat     attn_v        512  1024   4096
mul_mat     attn_output   512  4096   4096

# FFN gating (selects which experts to use)
mul_mat     ffn_gate_inp  512  8      4096

# MoE expert matmuls with realistic imbalanced routing
# Expert usage: [126, 323, 80, 68, 256, 37, 15, 119] tokens per expert
# Total: 1024 token-expert pairs (512 tokens × 2 experts/token)
# Expert 1 gets 21x more work than Expert 6!

mul_mat_id  ffn_gate_exp  512  14336  4096   8  2
expert_tokens  126, 323, 80, 68, 256, 37, 15, 119

mul_mat_id  ffn_up_exp    512  14336  4096   8  2
expert_tokens  126, 323, 80, 68, 256, 37, 15, 119

mul_mat_id  ffn_down_exp  512  4096   14336  8  2
expert_tokens  126, 323, 80, 68, 256, 37, 15, 119
